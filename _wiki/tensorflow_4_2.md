---
layout  : wiki
title   : 코딩야학 Tensorflow 4일차(2) 마지막
summary : 더 깊게 딥러닝의 시작, 히든레이어 적용하기
date    : 2020-08-30 16:33:45 +0900
updated : 2020-08-30 21:13:46 +0900
tags    : [ml, tensorflow, python, yahak, pandas]
comments: true
toc     : true
---

## 1. 보스턴 집값 예측에 히든레이어를 추가하기

이전시간에 보스턴 집값을 예측했던 것을 상기해보자.

해당 모델은 2개(**입력**, **출력**)의 층을가지고, 13개의 **독립변수**와 1개의 
**종속변수**로 총 14개의 노드로 구성된 모델로 구성되었다.

이번시간에는 입,출력층 사이에 **히든레이어**를 추가하여서 모델의 성능을 향상시켜 볼 것이다.


## 2. 코드로 작성해보자

```python
import tensorflow as tf
import pandas as pd

path = "https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/boston.csv"
boston = pd.read_csv(path)

dependent = boston[['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']]
independent = boston[['medv']]

X = tf.keras.layers.Input(shape=[13])
H = tf.keras.layers.Dense(10, activation='swish')(X)
Y = tf.keras.layers.Dense(1)(H)
model = tf.keras.models.Model(X, Y)
model.compile(loss = 'mse')

model.fit(dependent, independent, epochs=100)
```

기존의 boston 집값 예측 코드와 비교해보면, 노드 10개 짜리 H라는 층을 추가적으로 생성하였고, 활성화 함수로는 
`swish`를 사용하였다. Y층이 X층과 연결되는 것이 아닌 H층과 연결되게 변경하였다.

## 3. 결과 비교

기존의 히든레이어를 사용하지 않은 모델과, 히든레이어를 추가한 모델을 비교해보자!

보다 정확한 비교를 위해서, 각각의 모델에 `epochs=100`을 적용하여 10회 학습을 진행한 loss의 평균을 지표로 삼는다.

### 3.1 히든레이어를 사용하지 않은 모델의 loss값 (10개)

```
16/16 [==============================] - 0s 875us/step - loss: 51.8339
16/16 [==============================] - 0s 781us/step - loss: 56.4396
16/16 [==============================] - 0s 1ms/step - loss: 54.6764
16/16 [==============================] - 0s 800us/step - loss: 64.6523
16/16 [==============================] - 0s 960us/step - loss: 82.0111
16/16 [==============================] - 0s 745us/step - loss: 58.2649
16/16 [==============================] - 0s 803us/step - loss: 45.9726
16/16 [==============================] - 0s 790us/step - loss: 40.8678
16/16 [==============================] - 0s 814us/step - loss: 79.2484
16/16 [==============================] - 0s 1ms/step - loss: 47.4234
```
위의 값들의 평균을 계산해보니 약 **58.1390**이라는 값이 나왔다

### 3.2 히든레이어를 추가한 모델의 loss값 (10개)
```
16/16 [==============================] - 0s 841us/step - loss: 38.0614
16/16 [==============================] - 0s 1ms/step - loss: 31.1188
16/16 [==============================] - 0s 918us/step - loss: 33.6800
16/16 [==============================] - 0s 924us/step - loss: 43.0693
16/16 [==============================] - 0s 820us/step - loss: 35.9184
16/16 [==============================] - 0s 988us/step - loss: 32.2964
16/16 [==============================] - 0s 888us/step - loss: 33.7863
16/16 [==============================] - 0s 840us/step - loss: 29.7337
16/16 [==============================] - 0s 1ms/step - loss: 36.1669
16/16 [==============================] - 0s 906us/step - loss: 36.4696
```
평균을 계산해보니까 **35.0300**라는 수치가 나왔다. 기존의 값과 비교해보니 평균적으로 **23.109** 
만큼이나 loss 줄어든 것을 확인할 수 있었다.

## 4. 수업을 마치며

평소 관심은 있었지만, 너무 어렵게 생각해서 공부하는 것을 실천하지 못하고 있었습니다. 하지만 
코딩야학이라는 좋은 프로그램을 통해서 머신러닝이라는 분야를 가벼운 마음으로 경험해 볼 수 있는 좋은 기회였습니다. 
단순히 이번수업으로 끝내는 것이 아니라, 좀 더 머신러닝이라는 분야를 공부해 봐야겠다는 생각을 했습니다.

## 5. 더 학습할 내용
* `swish`, `softmax`, `cross entropy`등에 대해서 학습하기
* 히든레이어를 적용했을때, 학습력이 올라가는 이유에 대해서 조사하기

## Link

* [Youtube 봉수골 개발자 이선비](https://www.youtube.com/watch?v=dpw0wY13XDk&list=PLl1irxoYh2wyLwJutUZx5Q_QEEDZoXBnz&index=1)
